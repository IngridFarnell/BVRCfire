---
title: "FourFires-Analysis_mlr"
author: "Ingrid Farnell & Alana Clason"
date: "25/02/2022"
output: html_document
editor_options: 
  chunk_output_type: console
---

This script runs ranger using mlr3 package
More info: https://mlr3book.mlr-org.com/introduction.html

Overall data flow:
Step 1: Prepare data
Step 2: Define spatial task
Step 3: Define learner (ranger classification)
Step 4: Separate task into learning (80%) and test (20%) datasets
Step 5: Train learner on training data
Step 6: Optimize hyperparameters (ntrees & mtry) using spatial cross fold validation
Step 7: Predict test data using trained learner
Step 8: Evaluate model on test data
Step 9: Visualize 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#------------------------------ Load libraries---------------------------------#
ls <- c("tidyverse", "data.table", "magrittr") # Data Management and Manipul,ation
ls <- append(ls, c("raster")) # geo comp.
ls <- append(ls, c("pdp", "mlr3", "mlr3spatiotempcv", "mlr3verse", "vegan", "corrplot")) # analysis
ls <- append(ls, c("iml", "patchwork", "DALEX", "DALEXtra")) # model interpretation/visualization

# Install if needed -- then load. 
new.packages <- ls[!(ls %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
lapply(ls, library, character.only = TRUE)  # load the required packages
rm(ls, new.packages)


#------------------------------ 1. Load data ----------------------------------#
#SpatialFilesPath <- "E:/Ingrid/Borealis/BVRCfire"
SpatialFilesPath <- getwd()
# Set the fires of interest - all 2018 fires with openings
FiresOfInterest <- c("G41607", "G51632", "R11498", "R11796","R11921","R21721")


# Read in the rasters
variable_list <- list.files(paste0(SpatialFilesPath, "/Inputs/Rasters/"),
                            pattern =  paste(FiresOfInterest, sep = "", collapse = "|"),
                            recursive = TRUE,
                            full.names = TRUE)
variable_list <- grep("tif", variable_list, value=TRUE)
# Drop OpenID, None and SitePrepped
variable_list <- grep("OpenID|None|SitePrepped|n83", variable_list, value = TRUE, invert = TRUE)
# For now remove this one because the raster contains only NA's -- Alana to fix and then remove this line
variable_list <- grep("R11498_SpotBurn", variable_list, value = TRUE, invert = TRUE)
variable_list <- c(variable_list,paste0(SpatialFilesPath,"/Inputs/Rasters/Topography/DEM",
                                        c("aspect","hli","slope","tpi"),".tif"))

variables <- sapply(variable_list, raster)


# Rename the variables 
variable.name <- lapply(str_split(variable_list,"/"), function(x) grep(".tif", x, value=TRUE))
variable.name <- str_split(variable.name, ".tif", simplify = TRUE)[,1]

names(variables) <- variable.name

#ID the names of the categorical rasters
ctg_variables <- c("BEC", "BroadBurn", "Brushed", "DebrisMade", "DebrisPiled", "Fertil", "MechUnk", 
                   "OPENING_ID", "PileBurn", "Prune", "Soil", "Spaced", 
                   "SpotBurn", "WBurn")
CatRasts <- grep(paste(ctg_variables,sep = "", collapse = "|"),variable.name,value=TRUE)

```

Prepare the data:
- Resample response and predictor variables to have the same extent and resolution (30 x 30 m) for each of the fires. 
-     Categorical variables are resampled using nearest neighbourhood and continuous variables are resampled using bilinear          resampling.
- Subsample at 270-m grid spacing to reduce spatial autocorrelation
- Data must be free of NAs 
- Data must not have variables with no variance


```{r, include = FALSE, warning = FALSE}
# ------------------------------Prepare data-----------------------------------#
#--------------------------2a. Resample rasters and stack----------------------#

# Using base rasters resample response and predictor variables to same extent and resolution
for(i in 1:length(FiresOfInterest)){
  allFireRasts <- variables[c(grep(FiresOfInterest[i],variables),grep("DEM",variables))]
  baseFireRast <- allFireRasts[grep("Base",allFireRasts)][[1]] #index just makes it not a list
  allFireRasts <- allFireRasts[grep("Base",allFireRasts,invert=TRUE)]
  
  # Resample categorical and continuous variables differently
  a <- list()
  for(j in 1:length(allFireRasts)){
    if(names(allFireRasts[[j]]) %in% CatRasts){
      a[[j]] <- raster::resample(allFireRasts[[j]], baseFireRast, method = "ngb")
    } else {
      a[[j]] <- raster::resample(allFireRasts[[j]], baseFireRast, method = "bilinear")
    }
  }
  fireID <- str_extract(names(allFireRasts[1]),FiresOfInterest[i])
  SimpleRastnames <- str_remove(str_remove(names(allFireRasts),FiresOfInterest[i]),"_")
  names(a) <- SimpleRastnames
  #stack the simplified names and assign to fire id rast name
  assign(paste0(fireID,"rasts"), stack(a))
}


#-----------------------------2b. Get sample points----------------------------#
# Create index of raster stacks
RastStacks <- list(G41607rasts, G51632rasts, R11498rasts, R11796rasts, R11921rasts, R21721rasts)

for(i in 1:length(FiresOfInterest)){
  allFireRasts <- variables[grep(FiresOfInterest[i],variables)]
  dNBRFireRast <- allFireRasts[grep("dNBR",allFireRasts)][[1]] #use a raster (doesn't matter which one)
  
  # 270 m grid distance
  b <- aggregate(dNBRFireRast, fact = 9, fun = mean)
  points270 <- rasterToPoints(b, spatial = TRUE) # get sample grid: 1 point/270 m, spatial = TRUE so coordinates are attached
  colnames(points270@data) <- "drop" # make sure to drop this later on(it's a place holder column for points)
  
  # Extract response and predictor values at sample points
  SampledRaster <- raster::extract(RastStacks[[i]], points270, sp = TRUE)
  # Convert to data frame
  dat270 <- as.data.frame(SampledRaster) # hopefully xy = TRUE will attach coordinates, if not do sp = TRUE in above extract line
  
  # Drop rows that don't have an opening ID because we only want to include plantation openings
  dat270 <- dat270 %>% filter(!is.na(OPENING_ID))
  # Drop opening ID column
  dat270 <- subset(dat270, select =-c(OPENING_ID, drop))
  
  # Meet spatial RF requirements
  # 1. Must be free of NA
  dat270 <- dat270[complete.cases(dat270), ] # remove NAs
  
  # 2. Columns cannot have 0 variance
  RemoveZeroVar <- function(dat270) {
    dat270[, !sapply(dat270, function(x) min(x) == max(x))]
  }
  dat270 <- RemoveZeroVar(dat270)
  
  # 3. Columns must not yield NaN or Inf when scaled
  #sum(apply(scale(R11796dat270), 2, is.nan)) 
  #sum(apply(scale(R11796dat270), 2, is.infinite))
  # Find which columns are giving issue
  #sapply(as.data.frame(scale(R21721_270)), function(x)any(is.nan(x)))
  
  # Move response (dNBR) to first column
  dat270 <- dat270 %>% dplyr::select("dNBR", everything())
  fireID <- str_extract(names(allFireRasts[1]),FiresOfInterest[i])
  assign(paste0(fireID,"dat270"), dat270)
  
}
#watch - order is hard coded
list_dats <- list(G41607dat270,G51632dat270,R11498dat270,R11796dat270,R11921dat270,R21721dat270)
FiresOfInterest
for(ii in 1:length(list_dats)){
  write.csv(list_dats[[ii]],paste0("./Inputs/",FiresOfInterest[ii],"dat270.csv"),row.names = FALSE)
}

```


Load the above data that I saved to my computer (for faster processing)
Load data as sf object
- scale dNBR *1000
```{r, include=FALSE}
ctg_variables <- c("BEC", "BroadBurn", "Brushed", "DebrisMade", "DebrisPiled", "Fertil", "MechUnk", 
                   "OPENING_ID", "PileBurn", "Prune", "Soil", "Spaced", 
                   "SpotBurn", "WBurn","dNBRCAT")
datPath <-  "./Inputs/"  #"C:/Users/farne/Documents/" 

Chutanli <- fread(paste0(datPath,"G41607dat270.csv"))
Chutanli <- Chutanli %>%
  mutate_at((colnames(Chutanli)[colnames(Chutanli) %in% ctg_variables]), factor) %>%
  dplyr::select(-c("dNBRReSamp")) #can remove if you remembered to save without row.names
Chutanli[,dNBR := dNBR*1000]
Chutanli[HistoricFires==0 ,HistoricFires:=100]
Chutanli_dist <- dist(Chutanli[,.(x,y)], method = "euclidean")
Chutanli_pcnm <- pcnm(Chutanli_dist)
Chutanli[, c("PCNM1","PCNM2", "PCNM3",
             "PCNM4", "PCNM5","PCNM6") := .(Chutanli_pcnm$vectors[,"PCNM1"],
                                            Chutanli_pcnm$vectors[,"PCNM2"],
                                            Chutanli_pcnm$vectors[,"PCNM3"],
                                            Chutanli_pcnm$vectors[,"PCNM4"],
                                            Chutanli_pcnm$vectors[,"PCNM5"],
                                            Chutanli_pcnm$vectors[,"PCNM6"])]

Tezzeron <- fread(paste0(datPath,"G51632dat270.csv"))
Tezzeron <- Tezzeron %>%
  mutate_at((colnames(Tezzeron)[colnames(Tezzeron) %in% ctg_variables]), factor)%>%
  dplyr::select(-c("dNBRReSamp"))
Tezzeron[,dNBR := dNBR*1000]
Tezzeron_dist <- dist(Tezzeron[,.(x,y)], method = "euclidean")
Tezzeron_pcnm <- pcnm(Tezzeron_dist)
Tezzeron[, c("PCNM1","PCNM2", "PCNM3",
             "PCNM4", "PCNM5","PCNM6") := .(Tezzeron_pcnm$vectors[,"PCNM1"],
                                            Tezzeron_pcnm$vectors[,"PCNM2"],
                                            Tezzeron_pcnm$vectors[,"PCNM3"],
                                            Tezzeron_pcnm$vectors[,"PCNM4"],
                                            Tezzeron_pcnm$vectors[,"PCNM5"],
                                            Tezzeron_pcnm$vectors[,"PCNM6"])]

Shovel <- fread(paste0(datPath,"R11498dat270.csv"))
Shovel <- Shovel %>%
  mutate_at((colnames(Shovel)[colnames(Shovel) %in% ctg_variables]), factor)%>%
  dplyr::select(-c("dNBRReSamp"))
Shovel[,dNBR := dNBR*1000]
Shovel[HistoricFires==0 ,HistoricFires:=100]
Shovel_dist <- dist(Shovel[,.(x,y)], method = "euclidean")
Shovel_pcnm <- pcnm(Shovel_dist)
Shovel[, c("PCNM1","PCNM2", "PCNM3",
             "PCNM4", "PCNM5","PCNM6") := .(Shovel_pcnm$vectors[,"PCNM1"],
                                            Shovel_pcnm$vectors[,"PCNM2"],
                                            Shovel_pcnm$vectors[,"PCNM3"],
                                            Shovel_pcnm$vectors[,"PCNM4"],
                                            Shovel_pcnm$vectors[,"PCNM5"],
                                            Shovel_pcnm$vectors[,"PCNM6"])]

Verdun <- fread(paste0(datPath,"R11796dat270.csv"))
Verdun <- Verdun %>%
  mutate_at((colnames(Verdun)[colnames(Verdun) %in% ctg_variables]), factor)%>%
  dplyr::select(-c("dNBRReSamp"))
Verdun[,dNBR := dNBR*1000]
Verdun[HistoricFires==0 ,HistoricFires:=100]
Verdun_dist <- dist(Verdun[,.(x,y)], method = "euclidean")
Verdun_pcnm <- pcnm(Verdun_dist)
Verdun[, c("PCNM1","PCNM2", "PCNM3",
             "PCNM4", "PCNM5","PCNM6") := .(Verdun_pcnm$vectors[,"PCNM1"],
                                            Verdun_pcnm$vectors[,"PCNM2"],
                                            Verdun_pcnm$vectors[,"PCNM3"],
                                            Verdun_pcnm$vectors[,"PCNM4"],
                                            Verdun_pcnm$vectors[,"PCNM5"],
                                            Verdun_pcnm$vectors[,"PCNM6"])]

Island <- fread(paste0(datPath,"R11921dat270.csv"))
Island <- Island %>%
  mutate_at((colnames(Island)[colnames(Island) %in% ctg_variables]), factor)%>%
  dplyr::select(-c("dNBRReSamp"))
Island[,dNBR := dNBR*1000]
Island[HistoricFires==0 ,HistoricFires:=100]
Island_dist <- dist(Island[,.(x,y)], method = "euclidean")
Island_pcnm <- pcnm(Island_dist)
Island[, c("PCNM1","PCNM2", "PCNM3",
             "PCNM4", "PCNM5","PCNM6") := .(Island_pcnm$vectors[,"PCNM1"],
                                            Island_pcnm$vectors[,"PCNM2"],
                                            Island_pcnm$vectors[,"PCNM3"],
                                            Island_pcnm$vectors[,"PCNM4"],
                                            Island_pcnm$vectors[,"PCNM5"],
                                            Island_pcnm$vectors[,"PCNM6"])]

Nadina <- fread(paste0(datPath,"R21721dat270.csv"))
Nadina <- Nadina %>%
  mutate_at((colnames(Nadina)[colnames(Nadina) %in% ctg_variables]), factor)%>%
  dplyr::select(-c("dNBRReSamp"))
Nadina[,dNBR := dNBR*1000]
Nadina[HistoricFires==0 ,HistoricFires:=100]
Nadina_dist <- dist(Nadina[,.(x,y)], method = "euclidean")
Nadina_pcnm <- pcnm(Nadina_dist)
Nadina[, c("PCNM1","PCNM2", "PCNM3",
             "PCNM4", "PCNM5","PCNM6") := .(Nadina_pcnm$vectors[,"PCNM1"],
                                            Nadina_pcnm$vectors[,"PCNM2"],
                                            Nadina_pcnm$vectors[,"PCNM3"],
                                            Nadina_pcnm$vectors[,"PCNM4"],
                                            Nadina_pcnm$vectors[,"PCNM5"],
                                            Nadina_pcnm$vectors[,"PCNM6"])]

#ordisurf(Chutanli_xy, scores(Chutanli_pcnm, choices=1), bubble = 4, main = "PCNM 1")
#plot(Chutanli_pcnm$values) #most the variation is in the first 20 or so eigenvectors

#ggplot()+
 # geom_point(aes(y=Chutanli$dNBR, x= Chutanli_pcnm$vectors[,17]))+
  #geom_smooth(aes(y=Chutanli$dNBR, x= Chutanli_pcnm$vectors[,17]), method="gam")

#create datasets with variables to include in analysis. We are keeping them (cat response and continuous response) as seperate datasets, because it's imbedded in the code below to pass the entire object and not specify which columns to ignore
Chutanli_sf_con <- sf::st_as_sf(Chutanli[,-c("dNBRCAT")], coords = c("x", "y"))
Chutanli_sf_cat <- sf::st_as_sf(Chutanli[,-c("dNBR")], coords = c("x", "y"))

Tezzeron_sf_con <- sf::st_as_sf(Tezzeron[,-c("dNBRCAT")], coords = c("x", "y"))
Tezzeron_sf_cat <- sf::st_as_sf(Tezzeron[,-c("dNBR")], coords = c("x", "y"))

Shovel_sf_con <- sf::st_as_sf(Shovel[,-c("dNBRCAT")], coords = c("x", "y"))
Shovel_sf_cat <- sf::st_as_sf(Shovel[,-c("dNBR")], coords = c("x", "y"))

Verdun_sf_con <- sf::st_as_sf(Verdun[,-c("dNBRCAT")], coords = c("x", "y"))
Verdun_sf_cat <- sf::st_as_sf(Verdun[,-c("dNBR")], coords = c("x", "y"))

Island_sf_con <- sf::st_as_sf(Island[,-c("dNBRCAT")], coords = c("x", "y"))
Island_sf_cat <- sf::st_as_sf(Island[,-c("dNBR")], coords = c("x", "y"))

Nadina_sf_con <- sf::st_as_sf(Nadina[,-c("dNBRCAT")], coords = c("x", "y"))
Nadina_sf_cat <- sf::st_as_sf(Nadina[,-c("dNBR")], coords = c("x", "y"))

```


Remove highly correlated variables
```{r}
#--- Assess correlated covariates
plot <- list()
corrT <- list()
task_names <- c("Chutlani","Nadina","Shovel","Island","Verdun","Tezzeron")
dat_names <- list(Chutlani,Nadina,Shovel,Island,Verdun,Tezzeron)

for(i in 1:length(dat_names)){
  dat <- as.data.table(dat_names[[i]])
  dat <- dat %>% select_if(is.numeric)
  
  corrMat <- round(cor(dat, method="spearman"), 2)

  plot[[i]] <- corrplot(corrMat, 
                        title = paste0(task_names[i]))
  # 0.7 cutoff
  corr <- as.data.frame(corrMat)
  corr[abs(corr) < .7 & abs(corr) < 1] <- ""
  write.csv(corr, paste0(datPath, task_names[i], "corr.csv"))
}
#keep dmc, fwi. Drop dc, isi
```


I just moved this directly after the data cleaning to make this the spot we modify RF
```{r}
set.seed(456)

task_names <- c("Chutlani","Nadina","Shovel","Island","Verdun","Tezzeron")
dat_list <- list(Chutanli_sf_cat, Nadina_sf_cat, Shovel_sf_cat, Island_sf_cat,
                 Verdun_sf_cat, Tezzeron_sf_cat)

#CV_Conf <- list()
Test_Conf <- list()
TrainImp_dat <- list()
RF_scores <- list()
pd_scores <- list()
effects <- list()
NumCores <- 20

for(i in 1:length(task_names)){
  # Remove correlated covariates
  dat <- dat_list[[i]]
  dat[,c("dob", "bui", "dc", "isi")] <- list(NULL)
  
  task = TaskClassifST$new(task_names[i],
                         backend = dat_list[[i]], 
                         target = "dNBRCAT")
  
  #--- Step 2: Define a learner
  # default ranger parameters: https://mlr3learners.mlr-org.com/reference/mlr_learners_regr.ranger.html
  learner = lrn("classif.ranger",
                importance = "permutation",
                respect.unordered.factors = "order",
                predict_type = "response",
                num.threads=NumCores)
  
  #--- Step 3: Split stratified task into test/train data
  split = partition(task, ratio=0.8, stratify=TRUE) #so everytime we re-run, this selection will change. Can we make this the same everytime somehow? Or do we want the random sampling to vary everytime? - can use set.seed at begining 
  
  task_train = task$clone()$filter(rows=split$train)
  task_test = task$clone()$filter(rows=split$test)
  
  #--- Step 4: Feature selection (using nested resampling)
  measure = msr("classif.ce") #classification error
  fselector = fs("random_search") # using random_search b/c of large search space
  #level = 0.2 
  #terminator = trm("perf_reached", level=level)
  #terminator = trm("stagnation", iters=40, threshold=0)
  terminator = trm("evals", n_evals=20)
  inner_resampling = rsmp("spcv_coords", folds=5)
   
  optFtlrn = AutoFSelector$new(
    learner = learner,
    resampling = inner_resampling,
    measure = measure,
    terminator = terminator,
    fselector = fselector
  )
  
  #outer_resampling = rsmp("spcv_coords", folds=5)
  #autoplot(inner_resampling, task_train, fold_id = c(1:4), size = 0.7)
  
  #rr = mlr3::resample(task_train, optFtlrn, outer_resampling, store_models = TRUE)
  
  #--- Step 5: Model evaluation
  # outer folds
  #rr$score(measure)
  # Unbiased performance of model with optimal hyperparameters
  #rr$aggregate(measure)
  
  #So I think we have two model performance values here. The spatial cross-validation of the training data and then the performance on the test data
  
  #--- Step 6: Train final model (applies tuning)
  optFtlrn$train(task_train) #I don't see how the tuning is applied here
  #optFtlrn$model
  
  #--- Step 7: Predict on test data
  optFtlrnPred <- optFtlrn$predict(task_test) 
  optFtlrnPred$score(measure)
  
  #cross-validated predictions (only train data)
  #rr$prediction()
  #trained model predictions
  optFtlrnPred$prob
  
  #confusion matrix:
  #CV_Conf[[i]] <- rr$prediction()$confusion
  Test_Conf[[i]] <- optFtlrnPred$confusion
  
 # rr$score()
  RF_scores[[i]] <- optFtlrnPred$score()
  
  #sum(rr$prediction()$confusion) #so this is on the training data
  #sum(optFtlrnPred$confusion) # this is on the test data
  
  #it's good at predicting unburned, there is no high severity here
  
  #--- Step 8: Model visualization
  # Feature importance
  TrainImp_dat[[i]] <- as.data.table(optFtlrn$learner$importance(), keep.rownames = TRUE)
  
  # Feature Imp
  #       Level of importance of the features
  #AC: I think we need to pass just the training data?
  dat_train <- task_train$data()
  x_train <- dat_train[,-c("dNBRCAT")]
  #dat_test <- task_test$data()
  
  #impFt <- TrainImp_dat[[i]]$Feature
  model <- Predictor$new(optFtlrn, data = x_train, y = dat_train$dNBRCAT)
  #effect.imp <- FeatureImp$new(model, loss = "ce")
  #effect.imp$plot(features = impFt)
  
  effects[[i]] <- FeatureEffects$new(model)
  #plot(effects)
  #optFtlrn$predict_type <- "response"
  model_exp = explain_mlr3(optFtlrn,
                         data = x_train, # provide data without y 
                         y = dat_train$dNBRCAT,
                         colorize = FALSE,
                         type="classification")

  # Partial dependence plots of top variables
  pd_scores[[i]] <- model_profile(model_exp)$agr_profiles
  
}  
  
RF_scores
Test_Conf

#my attempt to make that multi-fire graph:
for(i in 1:6){
  TrainImp_dat[[i]][,FireID := task_names[i]]
}
TrainImp_dt <- rbindlist(TrainImp_dat)
setnames(TrainImp_dt, c("Variable","Score","FireID"))

#ggplot(TrainImp_dt, aes(x = reorder(V1, V2), y = V2)) +
ggplot(TrainImp_dt, aes(x = Score, y = reorder(Variable,Score))) +  
  geom_point(aes(colour=FireID), size=2) +
  xlab("")

ggplot(TrainImp_dt, aes(x = Score, y = reorder(Variable,Score))) +  
  geom_point(aes(colour=FireID), size=2) +
  facet_wrap("FireID")+
  xlab("")

Silvic_Vars <- c("BroadBurn", "Brushed", "DebrisMade", "DebrisPiled", "Fertil", "PileBurn", 
                  "Prune", "Soil", "Spaced", "SpotBurn")

Silv_Imp_dt <- TrainImp_dt[Variable %in% Silvic_Vars]
ggplot(Silv_Imp_dt, aes(x = Score, y = reorder(Variable,Score))) +  
  geom_point(aes(colour=FireID), size=2) +
  xlab("") +
  xlim(c(-0.01,0.2))+
  ylab("Variable Importance")

num_features <- c("PlantAge")

plot(effects[[1]], features = num_features) 
    #scale_y_continuous("Estimated dNBR") +
    ggtitle("Partial Dependence profiles for selected variables")

plot(effects[[1]]$results$PlantAge$.value ~ effects[[1]]$results$PlantAge$.borders)


ggplot(effects[[1]]$results$PlantAge)+
  geom_line(aes(x=.borders, y=.value, colour = .class), size=2)

PD_plantAge <- list() 
for(i in 1:length(effects)){
  PD_plantAge[[i]] <- data.table(effects[[i]]$results$PlantAge)[,FireID := task_names[i]]
}
PD_plantAge_dt <- rbindlist(PD_plantAge)

ggplot(PD_plantAge_dt)+
  geom_line(aes(x=.borders, y=.value, colour = FireID), size=1.5)+
  facet_wrap(".class")

PD_CrownClos <- list() 
for(i in 1:length(effects)){
  PD_CrownClos[[i]] <- data.table(effects[[i]]$results$CROWN_CLOS)[,FireID := task_names[i]]
}
PD_CrownClos_dt <- rbindlist(PD_CrownClos)

ggplot(PD_CrownClos_dt)+
  geom_line(aes(x=.borders, y=.value, colour = FireID), size=1.5)+
  facet_wrap(".class")


plot(pd_scores[[1]], variables = num_features) +
    #scale_y_continuous("Estimated dNBR") +
    ggtitle("Partial Dependence profiles for selected variables")
plot(pd_scores[[1]], pd_scores[[2]], variables=num_features, color="_label_")
```


# Chutanli
```{r Chutanli, echo=FALSE}


################################################################################
#################################### Categorical response variable

# Correlation
datCon_list <- Chutanli_sf_con
dat <- as.data.table(datCon_list)
dat <- dat %>% select_if(is.numeric)
corrMat <- round(cor(dat, method="spearman"), 2)

corrplot(corrMat)
# 0.7 cutoff
corr <- as.data.frame(corrMat)
corr[abs(corr) < .7 & abs(corr) < 1] <- ""
corr

#--- Step 1: Define a task
# dat <- as.data.table(Chutanli_sf_cat)
# dat <- dat[,-c("geometry")]
# x <- dat[,-c("dNBRCAT")]
# dat[,dNBRCAT:=as.factor(ifelse(dNBRCAT==1,0,1))]
#Chutanli_sf_cat <- dplyr::select(Chutanli_sf_cat, - PlantAge)
set.seed(456)


task = TaskClassifST$new("Chutanli",
                       backend = Chutanli_sf_cat, #dat, 
                       target = "dNBRCAT")

#--- Step 2: Define a learner
# default ranger parameters: https://mlr3learners.mlr-org.com/reference/mlr_learners_regr.ranger.html
learner = lrn("classif.ranger",
              importance = "permutation",
              respect.unordered.factors = "order",
              predict_type = "prob",
              num.threads=20)

#--- Step 3: Split stratified task into test/train data
split = partition(task, ratio=0.8, stratify=TRUE) #so everytime we re-run, this selection will change. Can we make this the same everytime somehow? Or do we want the random sampling to vary everytime? -- Set seed at the very beginning of workflow. partition() is a mlr3 function so set.seed at beginning appears to work. Ran, cleared, reran and splits were the same both times. 

task_train = task$clone()$filter(rows=split$train)
task_test = task$clone()$filter(rows=split$test)

#--- Step 4: Feature selection (using nested resampling)
measure = msr("classif.ce") #classification error
fselector = fs("random_search", batch_size=15) # using random_search b/c of large search space

level = 0.5 #0.1 # Alana try 0.1
terminator = trm("perf_reached", level=level) #fselector stops after model measure is within 10% of best model
# doesn't seem to terminate with level <0.5, anything higher and terminates right after 1 evaluation... but not sure what this means? unstable model? .. but maybe on Alana's computer it actually terminates in a reasonable time
inner_resampling = rsmp("spcv_coords", folds=5)
 
optFtlrn = AutoFSelector$new(
  learner = learner,
  resampling = inner_resampling,
  measure = measure,
  terminator = terminator,
  fselector = fselector
)

# outer_resampling = rsmp("spcv_coords", folds=5)
# autoplot(inner_resampling, task_train, fold_id = c(1:4), size = 0.7)
# 
# rr = mlr3::resample(task_train, optFtlrn, outer_resampling, store_models = TRUE)
# 
# #--- Step 5: Model evaluation
# # outer folds
# rr$score(measure)
# # Unbiased performance of model with optimal hyperparameters
# rr$aggregate(measure)

#So I think we have two model performance values here. The spatial cross-validation of the training data and then the performance on the test data - Yes, need to pick one or the other because if you run both, the test data model evaluation increases (because learner has been trained twice)

#--- Step 6: Train final model (applies tuning)
optFtlrn$train(task_train) 
optFtlrn$model

#--- Step 7: Predict on test data & assess model
optFtlrnPred = optFtlrn$predict(task_test) 
optFtlrnPred$score(measure)
# 
#cross-validated predictions (only train data)
# rr$prediction()
#trained model predictions
optFtlrnPred$prob

#confusion matrix:
# rr$prediction()$confusion
optFtlrnPred$confusion



# sum(rr$prediction()$confusion) #so this is on the training data
sum(optFtlrnPred$confusion) # this is on the test data

#it's good at predicting unburned, there is no high severity here

#--- Step 8: Model visualization
# Feature importance
importance = as.data.table(optFtlrn$learner$importance(), keep.rownames = TRUE)
colnames(importance) = c("Feature", "Importance")
ggplot(importance, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_col() + coord_flip() + xlab("")

# Feature Imp
#       Level of importance of the features
#AC: I think we need to pass just the training data?
dat_train <- task_train$data()
x_train <- dat_train[,-c("dNBRCAT")]
dat_test <- task_test$data()

impFt = importance$Feature
model <- Predictor$new(optFtlrn, data = x_train, y = dat_train$dNBRCAT)
effect.imp = FeatureImp$new(model, loss = "ce")
effect.imp$plot(features = impFt)



######################################################

###AC: I'm not sure about everything from here down

#--- using IML package
dat <- as.data.table(Chutanli_sf_cat)
x <- dat[,-c("dNBRCAT","geometry")]


model <- Predictor$new(optFtlrn, data = x_train, y = dat_train$dNBRCAT)

# or look at partial dependance plots
effect <- FeatureEffects$new(model, method = "pdp") # partial dependance plots
plot(effect)

#--- Compare training to test - AC: I'm not sure yet what this is doing exactly
# Feature Importance
# training
model.train = Predictor$new(optFtlrn, data = dat_train, y = "dNBRCAT")
effect = FeatureImp$new(model.train, loss = "ce")
plot_train = plot(effect, features = impFt) #this is different than the feature importance above

# test
model.test = Predictor$new(optFtlrn, data = dat_test, y = "dNBRCAT")
effect = FeatureImp$new(model.test, loss = "ce")
plot_test = plot(effect, features = impFt)

# combine into single plot
plot_train + plot_test

# Feature effects
# training
effect = FeatureEffects$new(model.train)
plot(effect, features = impFt)

# test
effect = FeatureEffects$new(model.train)
plot(effect, features = impFt)


#--- DALEX
# similar to IML package
#     Methods for analyzing the model at the level of a single prediction and the global level
model_exp = explain_mlr3(optFtlrn,
                         data = x, # provide data without y 
                         y = dat$dNBRCAT,
                         label = "Chutanli",
                         colorize = FALSE)

# Dataset level exploration
# Importance of variables - permutation based importance
model_vi = model_parts(model_exp)
head(model_vi)
plot(model_vi, show_boxplots = FALSE)

# Partial dependence plots of top variables
pd = model_profile(model_exp, 
                   variables = impFt)$agr_profiles
pd
plot(pd) +
  scale_y_continuous("Estimated dNBR") +
  ggtitle("Partial Dependence profiles for selected variables")

```

Relationship between plantation age and dNBR (continuous)
```{r}

set.seed(456)

#--- Assess correlated covariates
task_names <- c("Chutlani","Nadina","Shovel","Island","Verdun","Tezzeron")
dat_list <- list(Chutanli_sf_cat, Nadina_sf_cat, Shovel_sf_cat, Island_sf_cat,
                 Verdun_sf_cat, Tezzeron_sf_cat)

datCon_list <- list(Chutanli_sf_con, Nadina_sf_con, Shovel_sf_con, Island_sf_con,
                 Verdun_sf_con, Tezzeron_sf_con)

plot <- list()
corrT <- list()

for(i in 1:length(task_names)){
  dat <- as.data.table(datCon_list[[i]])
  dat <- dat %>% select_if(is.numeric)
  
  corrMat <- round(cor(dat, method="spearman"), 2)

  plot[[i]] <- corrplot(corrMat, 
                        title = paste0(task_names[i]))
  # 0.7 cutoff
  corr <- as.data.frame(corrMat)
  corr[abs(corr) < .7 & abs(corr) < 1] <- ""
  write.csv(corr, paste0(datPath, task_names[i], "corr.csv"))
}


#--- Run model  
CV_Conf <- list()
Test_Conf <- list()
TrainImp_dat <- list()
NumCores <- 20

for(i in 1:length(task_names)){
  # Remove correlated covariates
  dat <- dat_list[[i]]
  dat[,c("dob", "bui", "dc", "isi")] <- list(NULL)

  #--- Step 1: Define a task
  task = TaskClassifST$new(task_names[i],
                         backend = dat, 
                         target = "dNBRCAT")
  
  #--- Step 2: Define a learner
  # default ranger parameters: https://mlr3learners.mlr-org.com/reference/mlr_learners_regr.ranger.html
  learner = lrn("classif.ranger",
                importance = "permutation",
                respect.unordered.factors = "order",
                predict_type = "prob",
                num.threads=NumCores)
  
  #--- Step 3: Split stratified task into test/train data
  split = partition(task, ratio=0.8, stratify=TRUE) 
  
  task_train = task$clone()$filter(rows=split$train)
  task_test = task$clone()$filter(rows=split$test)
  
  #--- Step 4: Feature selection (using nested resampling)
  measure = msr("classif.ce") #classification error
  fselector = fs("random_search") # using random_search b/c of large search space
  level = 0.5 #0.1 Alana can you try 0.1 and see if it works?
  terminator = trm("perf_reached", level=level)
  inner_resampling = rsmp("spcv_coords", folds=5)
   
  optFtlrn = AutoFSelector$new(
    learner = learner,
    resampling = inner_resampling,
    measure = measure,
    terminator = terminator,
    fselector = fselector
  )

  #--- Step 5: Train final model (applies tuning)
  optFtlrn$train(task_train) #I don't see how the tuning is applied here
  #optFtlrn$model
  
  #--- Step 6: Predict on test data & evaluate model
  optFtlrnPred <- optFtlrn$predict(task_test) 
  optFtlrnPred$score(measure)
 
  #trained model predictions
  optFtlrnPred$prob
  
  #confusion matrix:
  Test_Conf[[i]] <- optFtlrnPred$confusion
  
  sum(optFtlrnPred$confusion) # this is on the test data
  
  #it's good at predicting unburned, there is no high severity here
  
  #--- Step 8: Model visualization
  # Feature importance
  TrainImp_dat[[i]] <- as.data.table(optFtlrn$learner$importance(), keep.rownames = TRUE)
  
  # Feature Imp
  #       Level of importance of the features
  #AC: I think we need to pass just the training data?
  #dat_train <- task_train$data()
  #x_train <- dat_train[,-c("dNBRCAT")]
  #dat_test <- task_test$data()
  
  #impFt <- TrainImp_dat[[i]]$Feature
  #model <- Predictor$new(optFtlrn, data = x_train, y = dat_train$dNBRCAT)
  #effect.imp <- FeatureImp$new(model, loss = "ce")
  #effect.imp$plot(features = impFt)
}  
  


#CV_Conf
Test_Conf

ggplot(TrainImp_dat[[4]], aes(x = reorder(V1, V2), y = V2)) +
  geom_col() +
  coord_flip() + 
  xlab("")


#my attempt to make that multi-fire graph:
TrainImp_dat[[1]][,FireID:=task_names[1]]
TrainImp_dat[[2]][,FireID:=task_names[2]]

ggplot(TrainImp_dat[[2]], aes(x = reorder(V1, V2), y = V2)) +
  geom_point() +
  coord_flip() + 
  xlab("")
  
# Merge TrainImp_dat into one datatable
TrainImp_Alldat <- unique(rbindlist(TrainImp_dat, fill=TRUE), by="V1")
TrainImp_Alldat <- TrainImp_Alldat$V1



```

