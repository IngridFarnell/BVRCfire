---
title: "FourFires-Analysis_mlr"
author: "Ingrid Farnell"
date: "25/02/2022"
output: html_document
---

This script runs ranger using mlr3 package
More info: https://mlr3book.mlr-org.com/introduction.html

Overall data flow:
Step 1: Prepare data
Step 2: Create spatial task
Step 3: Create learner (ranger regression)
Step 4: Model cross-validation, assess model
Step 5: Train model & get R2(oob)
Step 6: Tune feature selection (tune model)
Step 7: Create new task with filtered features
Step 8: Model cross-validation on tuned model
Step 9: Train tuned model & get R2(oob)
Step 10: Decide whether to use original or tuned model
Step 11: Model interpretation - variable importance, partial dependence plots

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#------------------------------ Load libraries---------------------------------#
ls <- c("tidyverse", "data.table", "magrittr") # Data Management and Manipulation
ls <- append(ls, c("raster")) # geo comp.,
ls <- append(ls, c("pdp", "mlr3", "mlr3spatiotempcv", "mlr3verse")) # analysis
ls <- append(ls, c("iml", "patchwork", "DALEX", "DALEXtra")) # model interpretation/visualization

# Install if needed -- then load. 
new.packages <- ls[!(ls %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
lapply(ls, library, character.only = TRUE)  # load the required packages
rm(ls, new.packages)


#------------------------------ 1. Load data ----------------------------------#
SpatialFilesPath <- "E:/Ingrid/Borealis/BVRCfire"
#SpatialFilesPath <- getwd()
# Set the fires of interest - all 2018 fires with openings
FiresOfInterest <- c("G41607", "G51632", "R11498", "R11796","R11921","R21721")


# Read in the rasters
variable_list <- list.files(paste0(SpatialFilesPath, "/Inputs/Rasters/"),
                            pattern =  paste(FiresOfInterest, sep = "", collapse = "|"), # only import the fires of interest
                            recursive = TRUE,
                            full.names = TRUE)
variable_list <- grep("tif", variable_list, value=TRUE)
# Drop OpenID, None and SitePrepped
variable_list <- grep("OpenID|None|SitePrepped", variable_list, value = TRUE, invert = TRUE)
# For now remove this one because the raster contains only NA's -- Alana to fix and then remove this line
variable_list <- grep("R11498_SpotBurn", variable_list, value = TRUE, invert = TRUE)

variables <- sapply(variable_list, raster)


# Rename the variables 
variable.name <- lapply(str_split(variable_list,"/"), function(x) grep(".tif", x, value=TRUE))
variable.name <- str_split(variable.name, ".tif", simplify = TRUE)[,1]

names(variables) <- variable.name

#ID the names of the categorical rasters
ctg_variables <- c("BEC", "BroadBurn", "Brushed", "DebrisMade", "DebrisPiled", "Fertil", "MechUnk", 
                   "OPENING_ID", "PileBurn", "Prune", "Soil", "Spaced", 
                   "SpotBurn", "WBurn")
CatRasts <- grep(paste(ctg_variables,sep = "", collapse = "|"),variable.name,value=TRUE)

```

Prepare the data:
- Resample response and predictor variables to have the same extent and resolution (30 x 30 m) for each of the fires. 
-     Categorical variables are resampled using nearest neighbourhood and continuous variables are resampled using bilinear          resampling.
- Subsample at 270-m grid spacing to reduce spatial autocorrelation
- Data must be free of NAs 
- Data must not have variables with no variance


```{r, include = FALSE, warning = FALSE}
# ------------------------------Prepare data-----------------------------------#
#--------------------------2a. Resample rasters and stack----------------------#

# Using base rasters resample response and predictor variables to same extent and resolution
for(i in 1:length(FiresOfInterest)){
  allFireRasts <- variables[grep(FiresOfInterest[i],variables)]
  baseFireRast <- allFireRasts[grep("Base",allFireRasts)][[1]] #index just makes it not a list
  allFireRasts <- allFireRasts[grep("Base",allFireRasts,invert=TRUE)]
  
  # Resample categorical and continuous variables differently
  a <- list()
  for(j in 1:length(allFireRasts)){
    if(names(allFireRasts[[j]]) %in% CatRasts){
      a[[j]] <- resample(allFireRasts[[j]], baseFireRast, method = "ngb")
    } else {
      a[[j]] <- resample(allFireRasts[[j]], baseFireRast, method = "bilinear")
    }
  }
  fireID <- str_extract(names(allFireRasts[1]),FiresOfInterest[i])
  SimpleRastnames <- str_remove(str_remove(names(allFireRasts),FiresOfInterest[i]),"_")
  names(a) <- SimpleRastnames
  #stack the simplified names and assign to fire id rast name
  assign(paste0(fireID,"rasts"), stack(a))
}


#-----------------------------2b. Get sample points----------------------------#
# Create index of raster stacks
RastStacks <- list(G41607rasts, G51632rasts, R11498rasts, R11796rasts, R11921rasts, R21721rasts)

for(i in 1:length(FiresOfInterest)){
  allFireRasts <- variables[grep(FiresOfInterest[i],variables)]
  dNBRFireRast <- allFireRasts[grep("dNBR",allFireRasts)][[1]] #use a raster (doesn't matter which one)
  
  # 270 m grid distance
  b <- aggregate(dNBRFireRast, fact = 9, fun = mean)
  points270 <- rasterToPoints(b, spatial = TRUE) # get sample grid: 1 point/270 m, spatial = TRUE so coordinates are attached
  colnames(points270@data) <- "drop" # make sure to drop this later on(it's a place holder column for points)
  
  # Extract response and predictor values at sample points
  SampledRaster <- raster::extract(RastStacks[[i]], points270, sp = TRUE)
  # Convert to data frame
  dat270 <- as.data.frame(SampledRaster) # hopefully xy = TRUE will attach coordinates, if not do sp = TRUE in above extract line
  
  # Drop rows that don't have an opening ID because we only want to include plantation openings
  dat270 <- dat270 %>% filter(!is.na(OPENING_ID))
  # Drop opening ID column
  dat270 <- subset(dat270, select =-c(OPENING_ID, drop))
  
  # Meet spatial RF requirements
  # 1. Must be free of NA
  dat270 <- dat270[complete.cases(dat270), ] # remove NAs
  
  # 2. Columns cannot have 0 variance
  RemoveZeroVar <- function(dat270) {
    dat270[, !sapply(dat270, function(x) min(x) == max(x))]
  }
  dat270 <- RemoveZeroVar(dat270)
  
  # 3. Columns must not yield NaN or Inf when scaled
  #sum(apply(scale(R11796dat270), 2, is.nan)) 
  #sum(apply(scale(R11796dat270), 2, is.infinite))
  # Find which columns are giving issue
  #sapply(as.data.frame(scale(R21721_270)), function(x)any(is.nan(x)))
  
  # Move response (dNBR) to first column
  dat270 <- dat270 %>% dplyr::select("dNBR", everything())
  fireID <- str_extract(names(allFireRasts[1]),FiresOfInterest[i])
  assign(paste0(fireID,"dat270"), dat270)
  
}

```


Load the above data that I saved to my computer (for faster processing)
Load data as sf object
- scale dNBR *1000
```{r, include=FALSE}
ctg_variables <- c("BEC", "BroadBurn", "Brushed", "DebrisMade", "DebrisPiled", "Fertil", "MechUnk", 
                   "OPENING_ID", "PileBurn", "Prune", "Soil", "Spaced", 
                   "SpotBurn", "WBurn")

Chutanli <- fread("C:/Users/farne/Documents/G41607dat270.csv")
Chutanli <- Chutanli %>%
  mutate_at((colnames(Chutanli)[colnames(Chutanli) %in% ctg_variables]), factor)
Chutanli[,dNBR := dNBR*1000]

Tezzeron <- fread("C:/Users/farne/Documents/G51632dat270.csv")
Tezzeron <- Tezzeron %>%
  mutate_at((colnames(Tezzeron)[colnames(Tezzeron) %in% ctg_variables]), factor)
Tezzeron[,dNBR := dNBR*1000]

Shovel <- fread("C:/Users/farne/Documents/R11498dat270.csv")
Shovel <- Shovel %>%
  mutate_at((colnames(Shovel)[colnames(Shovel) %in% ctg_variables]), factor)
Shovel[,dNBR := dNBR*1000]

Verdun <- fread("C:/Users/farne/Documents/R11796dat270.csv")
Verdun <- Verdun %>%
  mutate_at((colnames(Verdun)[colnames(Verdun) %in% ctg_variables]), factor)
Verdun[,dNBR := dNBR*1000]

Island <- fread("C:/Users/farne/Documents/R11921dat270.csv")
Island <- Island %>%
  mutate_at((colnames(Island)[colnames(Island) %in% ctg_variables]), factor)
Island[,dNBR := dNBR*1000]

Nadina <- fread("C:/Users/farne/Documents/R21721dat270.csv")
Nadina <- Nadina %>%
  mutate_at((colnames(Nadina)[colnames(Nadina) %in% ctg_variables]), factor)
Nadina[,dNBR := dNBR*1000]

Chutanli_sf = sf::st_as_sf(Chutanli, coords = c("x", "y"))
Tezzeron_sf = sf::st_as_sf(Tezzeron, coords = c("x", "y"))
Shovel_sf = sf::st_as_sf(Shovel, coords = c("x", "y"))
Verdun_sf = sf::st_as_sf(Verdun, coords = c("x", "y"))
Island_sf = sf::st_as_sf(Island, coords = c("x", "y"))
Nadina_sf = sf::st_as_sf(Nadina, coords = c("x", "y"))

```


# Chutanli
Train a ranger model then do model interpretation
```{r Chutanli, echo=FALSE}

#--- Step 1: Define a task
task = TaskRegrST$new("Chutanli",
                       backend = Chutanli_sf, 
                       target = "dNBR")


#--- Step 2: Define a learner
# default ranger parameters: https://mlr3learners.mlr-org.com/reference/mlr_learners_regr.ranger.html
learner = lrn("regr.ranger", 
              num.trees = 1000,
              importance = "permutation",
              respect.unordered.factors = "order",
              predict_type = "response")


#--- Step 3: Model validation
# lists different performance measures https://mlr3.mlr-org.com/reference/mlr_measures.html 
resampling = rsmp("repeated_spcv_coords", folds=10, repeats=2)
rr = resample(task, learner, resampling, store_models = TRUE, store_backends = FALSE)
rr$aggregate(msr("regr.rsq")) # fold R2 (avg. perforamance over all k fold from CV)
rr$aggregate(msr("regr.mse")) # mean square error
rr$aggregate(msr("oob_error")) # out-of-bag-error
rr$prediction()$score(msr("regr.rsq")) # global R2 (R2 over every data point predicted during CV)
# fold R2 = 0.19
# global R2 = 0.24


#--- Step 4: Train learner
learner$train(task)
learner$model # R2 on fully trained model
#R2oob = 0.34


#--- Step 5: Feature selection
resampling = rsmp("spcv_coords") 
measure = msr("regr.rsq")
terminator = trm("evals", n_evals=20)

instance = FSelectInstanceSingleCrit$new(
  task = task,
  learner = learner,
  resampling = resampling,
  measure = measure, 
  terminator = terminator
)

fselector = fs("random_search")
fselector$optimize(instance)
instance$result_feature_set


#--- Step 5: Subset task with selected features
task$select(instance$result_feature_set) # subsets task, so need to reread original if want to use


#--- Step 6: Tuned model validation
resampling = rsmp("repeated_spcv_coords", folds=10, repeats=2)
rrFlt = resample(task, learner, resampling, store_models = TRUE, store_backends = FALSE)
rrFlt$aggregate(msr("regr.rsq")) # fold
rrFlt$prediction()$score(msr("regr.rsq")) # global
# fold R2 = 0.15
# global R2 = 0.22


#--- Step 7: Train learner
learner$train(task)
learner$model # R2 on fully trained model
#R2oob = 0.34

#--- Step 8: Model interpretation
#--- using IML package
data = as.data.frame(Chutanli_sf) # IML package can't handle sf objects
x = data[which(names(data) != c("dNBR","geometry"))] 
model = Predictor$new(learner, data = x, y = data$dNBR)

# Feature Effects - ale plots
#      computes the effects for all given features on the model prediction
#      ale plots > pdp plots because they still work when features are correlated
effect = FeatureEffects$new(model, method = "ale") # accumulated local effects 
plot(effect)

# or look at partial dependance plots
effect = FeatureEffects$new(model, method = "pdp") # partial dependance plots
plot(effect)

# Shapley
#      computes feature contributions for single predictions with the Shapley value
#      Given the current set of feature values, the contribution of a feature value to the difference between the actual              prediction and the mean prediction is the estimated Shapley value
x.interest = data.frame(x[1,])  
shapley = Shapley$new(model, x.interest = x.interest)
plot(shapley) # phi gives increase or decrease in probability given the values on vertical axis

# Feature Imp
#       Level of importance of the features
# I input the below variables before I did the variable permutation filter - so will likely have to change these as some of these variables were removed from the task in step 5
num_features = c("DOBrounded", "PlantAge", "BASAL_AREA", "CROWN_CLOS", "PineCov", "SpruceCov", "BroadBurn", "Brushed")
effect.imp = FeatureImp$new(model, loss = "mae") # mean absolute error
effect.imp$plot(features = num_features)

# Independent test data
train_set = sample(task$nrow, 0.8 * task$nrow) # train on 80% of data
test_set = setdiff(seq_len(task$nrow), train_set)
learner$train(task, row_ids = train_set)
prediction = learner$predict(task, row_ids = test_set)

# plot on training
model = Predictor$new(learner, data = (data[train_set,][which(names(data) != "geometry")]), y = "dNBR")
effect = FeatureImp$new(model, loss = "mae")
plot_train = plot(effect, features = num_features)

# plot on test data
model = Predictor$new(learner, data = (data[test_set,][which(names(data) != "geometry")]), y = "dNBR")
effect = FeatureImp$new(model, loss = "mae")
plot_test = plot(effect, features = num_features)

# combine into single plot
plot_train + plot_test

# Feature effects
model = Predictor$new(learner, data = (data[train_set,][which(names(data) != "geometry")]), y = "dNBR")
effect = FeatureEffects$new(model)
plot(effect, features = num_features)

model = Predictor$new(learner, data = (data[test_set,][which(names(data) != "geometry")]), y = "dNBR")
effect = FeatureEffects$new(model)
plot(effect, features = num_features)


#--- DALEX
# similar to IML package
#     Methods for analyzing the model at the level of a single prediction and the global level
model_exp = explain_mlr3(learner,
                         data = x, # provide data without y 
                         y = data$dNBR,
                         label = "Chutanli",
                         colorize = FALSE)

# Dataset level exploration
# Importance of variables - permutation based importance
model_vi = model_parts(model_exp)
head(model_vi)
plot(model_vi, show_boxplots = FALSE)

# Partial dependence plots of top variables
selected_variables =c("PlantAge", "PineCov", "CROWN_CLOS", "BASAL_AREA", "SpruceCov", "conifCov", "decidCov", "Brushed")
pd = model_profile(model_exp, 
                   variables = selected_variables)$agr_profiles
pd
plot(pd) +
  scale_y_continuous("Estimated dNBR") +
  ggtitle("Partial Dependence profiles for selected variables")


```

# Tezzeron
Train a ranger model then do model interpretation
```{r Tezzeron, echo=FALSE}
rm(learner, rr, rrFlt)

#--- Step 1: Define a task
task = TaskRegrST$new("Tezzeron",
                       backend = Tezzeron_sf, 
                       target = "dNBR")


#--- Step 2: Define a learner
# default ranger parameters: https://mlr3learners.mlr-org.com/reference/mlr_learners_regr.ranger.html
learner = lrn("regr.ranger", 
              num.trees = 1000,
              importance = "permutation",
              respect.unordered.factors = "order",
              predict_type = "response")


#--- Step 3: Model validation
resampling = rsmp("repeated_spcv_coords", folds=10, repeats=2)
rr = resample(task, learner, resampling, store_models = TRUE, store_backends = FALSE)
rr$aggregate(msr("regr.rsq")) # by fold R2 (avg. perforamance over all k fold from CV)
rr$prediction()$score(msr("regr.rsq")) # global R2 (R2 over every data point predicted during CV)
# fold R2 = -0.24
# global R2 = 0.53


#--- Step 4: Train learner
learner$train(task)
learner$model # R2 on fully trained model
#R2oob = 0.70


#--- Step 5: Feature selection
resampling = rsmp("spcv_coords") 
measure = msr("regr.rsq")
terminator = trm("evals", n_evals=20)

instance = FSelectInstanceSingleCrit$new(
  task = task,
  learner = learner,
  resampling = resampling,
  measure = measure, 
  terminator = terminator
)

fselector = fs("random_search")
fselector$optimize(instance)
instance$result_feature_set


#--- Step 5: Subset task with selected features
task$select(instance$result_feature_set) # this subsets task, so need to reread original task if want to use it after this


#--- Step 6: Tuned model validation
resampling = rsmp("repeated_spcv_coords", folds=10, repeats=2)
rrFlt = resample(tas, learner, resampling, store_models = TRUE, store_backends = FALSE)
rrFlt$aggregate(msr("regr.rsq")) # fold
rrFlt$prediction()$score(msr("regr.rsq")) # global
# fold R2 = -0.06
# global R2 = 0.56


#--- Step 7: Train learner
learner$train(task)
learner$model # R2 on fully trained model
#R2oob = 0.45


#--- Step 8: Model interpretation
#--- IML
data = as.data.frame(Tezzeron_sf)
x = data[which(names(data) != c("dNBR","geometry"))] # hopefully this works, removing geometry
model = Predictor$new(learner, data = x, y = data$dNBR)

# Feature Effects - ale plots
#      computes the effects for all given features on the model prediction
#      ale plots > pdp plots because they still work when features are correlated
effect = FeatureEffects$new(model, method = "ale") # accumulated local effects
plot(effect)

# Shapley
#      computes feature contributions for single predictions with the Shapley value
#      Given the current set of feature values, the contribution of a feature value to the difference between the actual              prediction and the mean prediction is the estimated Shapley value
x.interest = data.frame(x[1,])  
shapley = Shapley$new(model, x.interest = x.interest)
plot(shapley) # phi gives increase or decrease in probability given the values on vertical axis

# Feature Imp
#       Level of importance of the features
num_features = c("DOBrounded", "PlantAge", "BASAL_AREA", "CROWN_CLOS", "PineCov", "SpruceCov", "BroadBurn", "Brushed")
effect.imp = FeatureImp$new(model, loss = "mae") # mean absolute error
effect.imp$plot(features = num_features)

# Independent test data
train_set = sample(task$nrow, 0.8 * task$nrow)
test_set = setdiff(seq_len(task$nrow), train_set)
learner$train(task, row_ids = train_set)
prediction = learner$predict(task, row_ids = test_set)

# plot on training
model = Predictor$new(learner, data = (data[train_set,][which(names(data) != "geometry")]), y = "dNBR")
effect = FeatureImp$new(model, loss = "mae")
plot_train = plot(effect, features = num_features)

# plot on test data
model = Predictor$new(learner, data = (data[test_set,][which(names(data) != "geometry")]), y = "dNBR")
effect = FeatureImp$new(model, loss = "mae")
plot_test = plot(effect, features = num_features)

# combine into single plot
plot_train + plot_test

# Feature effects
model = Predictor$new(learner, data = (data[train_set,][which(names(data) != "geometry")]), y = "dNBR")
effect = FeatureEffects$new(model)
plot(effect, features = num_features)

model = Predictor$new(learner, data = (data[test_set,][which(names(data) != "geometry")]), y = "dNBR")
effect = FeatureEffects$new(model)
plot(effect, features = num_features)


#--- DALEX
#     Methods for analyzing the model at the level of a single prediction and the global level
model_exp = explain_mlr3(learner,
                         data = x, # provide data without y 
                         y = data$dNBR,
                         label = "Tezzeron",
                         colorize = FALSE)

# Dataset level exploration
# Importance of variables - permutation based importance
model_vi = model_parts(model_exp)
head(model_vi)
plot(model_vi, show_boxplots = FALSE)

# Partial dependence plots of top variables
selected_variables =c("PlantAge", "PineCov", "CROWN_CLOS", "BASAL_AREA", "SpruceCov", "conifCov", "decidCov", "Brushed")
pd = model_profile(model_exp, 
                   variables = selected_variables)$agr_profiles
pd
plot(pd) +
  scale_y_continuous("Estimated dNBR") +
  ggtitle("Partial Dependence profiles for selected variables")


```

# Shovel
Train a ranger model then do model interpretation
```{r Shovel, echo=FALSE}
rm(learner, rr, rrFlt)

#--- Step 1: Define a task
task = TaskRegrST$new("Shovel",
                       backend = Shovel_sf, 
                       target = "dNBR")


#--- Step 2: Define a learner
# default ranger parameters: https://mlr3learners.mlr-org.com/reference/mlr_learners_regr.ranger.html
learner = lrn("regr.ranger", 
              num.trees = 1000,
              importance = "permutation",
              respect.unordered.factors = "order",
              predict_type = "response")


#--- Step 3: Model validation
resampling = rsmp("repeated_spcv_coords", folds=10, repeats=2)
rr = resample(task, learner, resampling, store_models = TRUE, store_backends = FALSE)
rr$aggregate(msr("regr.rsq")) # by fold R2 (avg. perforamance over all k fold from CV)
rr$prediction()$score(msr("regr.rsq")) # global R2 (R2 over every data point predicted during CV)
# fold R2 = -0.067
# global R2 = 0.027


#--- Step 4: Train learner
learner$train(task)
learner$model # R2 on fully trained model
#R2oob = 0.38


#--- Step 5: Feature selection
resampling = rsmp("spcv_coords") 
measure = msr("regr.rsq")
terminator = trm("evals", n_evals=20)

instance = FSelectInstanceSingleCrit$new(
  task = task,
  learner = learner,
  resampling = resampling,
  measure = measure, 
  terminator = terminator
)

fselector = fs("random_search")
fselector$optimize(instance)
instance$result_feature_set


#--- Step 5: Subset task with selected features
task$select(instance$result_feature_set) # this subsets task, so need to reread original task if want to use it after this


#--- Step 6: Tuned model validation
resampling = rsmp("repeated_spcv_coords", folds=10, repeats=2)
rrFlt = resample(task, learner, resampling, store_models = TRUE, store_backends = FALSE)
rrFlt$aggregate(msr("regr.rsq")) # fold
rrFlt$prediction()$score(msr("regr.rsq")) # global
# fold R2 = 0.022
# global R2 = 0.081


#--- Step 7: Train learner
learner$train(task)
learner$model # R2 on fully trained model
#R2oob = 0.45


#--- Step 3: Model interpretation
#--- IML
data = as.data.frame(Shovel_sf)
x = data[which(names(data) != c("dNBR","geometry"))] # hopefully this works, removing geometry
model = Predictor$new(learner, data = x, y = data$dNBR)

# Feature Effects - ale plots
#      computes the effects for all given features on the model prediction
#      ale plots > pdp plots because they still work when features are correlated
effect = FeatureEffects$new(model, method = "ale") # accumulated local effects
plot(effect)

# Shapley
#      computes feature contributions for single predictions with the Shapley value
#      Given the current set of feature values, the contribution of a feature value to the difference between the actual              prediction and the mean prediction is the estimated Shapley value
x.interest = data.frame(x[1,])  
shapley = Shapley$new(model, x.interest = x.interest)
plot(shapley) # phi gives increase or decrease in probability given the values on vertical axis

# Feature Imp
#       Level of importance of the features
num_features = c("DOBrounded", "PlantAge", "BASAL_AREA", "CROWN_CLOS", "PineCov", "SpruceCov", "BroadBurn", "Brushed")
effect.imp = FeatureImp$new(model, loss = "mae") # mean absolute error
effect.imp$plot(features = num_features)

# Independent test data
train_set = sample(task$nrow, 0.8 * task$nrow)
test_set = setdiff(seq_len(task$nrow), train_set)
learner$train(task, row_ids = train_set)
prediction = learner$predict(task, row_ids = test_set)

# plot on training
model = Predictor$new(learner, data = (data[train_set,][which(names(data) != "geometry")]), y = "dNBR")
effect = FeatureImp$new(model, loss = "mae")
plot_train = plot(effect, features = num_features)

# plot on test data
model = Predictor$new(learner, data = (data[test_set,][which(names(data) != "geometry")]), y = "dNBR")
effect = FeatureImp$new(model, loss = "mae")
plot_test = plot(effect, features = num_features)

# combine into single plot
plot_train + plot_test

# Feature effects
model = Predictor$new(learner, data = (data[train_set,][which(names(data) != "geometry")]), y = "dNBR")
effect = FeatureEffects$new(model)
plot(effect, features = num_features)

model = Predictor$new(learner, data = (data[test_set,][which(names(data) != "geometry")]), y = "dNBR")
effect = FeatureEffects$new(model)
plot(effect, features = num_features)


#--- DALEX
#     Methods for analyzing the model at the level of a single prediction and the global level
model_exp = explain_mlr3(learner,
                         data = x, # provide data without y 
                         y = data$dNBR,
                         label = "Shovel",
                         colorize = FALSE)

# Dataset level exploration
# Importance of variables - permutation based importance
model_vi = model_parts(model_exp)
head(model_vi)
plot(model_vi, show_boxplots = FALSE)

# Partial dependence plots of top variables
selected_variables =c("PlantAge", "PineCov", "CROWN_CLOS", "BASAL_AREA", "SpruceCov", "conifCov", "decidCov", "Brushed")
pd = model_profile(model_exp, 
                   variables = selected_variables)$agr_profiles
pd
plot(pd) +
  scale_y_continuous("Estimated dNBR") +
  ggtitle("Partial Dependence profiles for selected variables")


```

# Verdun
Train a ranger model then do model interpretation
```{r Verdun, echo=FALSE}
rm(learner, rr, rrFlt)

#--- Step 1: Define a task
task = TaskRegrST$new("Verdun",
                       backend = Verdun_sf, 
                       target = "dNBR")


#--- Step 2: Define a learner
# default ranger parameters: https://mlr3learners.mlr-org.com/reference/mlr_learners_regr.ranger.html
learner = lrn("regr.ranger", 
              num.trees = 1000,
              importance = "permutation",
              respect.unordered.factors = "order",
              predict_type = "response")


#--- Step 3: Model validation
resampling = rsmp("repeated_spcv_coords", folds=10, repeats=2)
rr = resample(task, learner, resampling, store_models = TRUE, store_backends = FALSE)
rr$aggregate(msr("regr.rsq")) # by fold R2 (avg. perforamance over all k fold from CV)
rr$prediction()$score(msr("regr.rsq")) # global R2 (R2 over every data point predicted during CV)
# fold R2 = -0.070
# global R2 = 0.088


#--- Step 4: Train learner
learner$train(task)
learner$model # R2 on fully trained model
#R2oob = 0.45


#--- Step 5: Feature selection
resampling = rsmp("spcv_coords") 
measure = msr("regr.rsq")
terminator = trm("evals", n_evals=20)

instance = FSelectInstanceSingleCrit$new(
  task = task,
  learner = learner,
  resampling = resampling,
  measure = measure, 
  terminator = terminator
)

fselector = fs("random_search")
fselector$optimize(instance)
instance$result_feature_set


#--- Step 5: Subset task with selected features
task$select(instance$result_feature_set) # this still subsets task, so need to reread original task if want to use it after this


#--- Step 6: Tuned model validation
resampling = rsmp("repeated_spcv_coords", folds=10, repeats=2)
rrFlt = resample(task, learner, resampling, store_models = TRUE, store_backends = FALSE)
rrFlt$aggregate(msr("regr.rsq")) # fold
rrFlt$prediction()$score(msr("regr.rsq")) # global
# fold R2 = 0.028
# global R2 = 0.13


#--- Step 7: Train learner
learner$train(task)
learner$model # R2 on fully trained model
#R2oob = 0.46


#--- Step 8: Model interpretation
#--- IML
data = as.data.frame(Verdun_sf)
x = data[which(names(data) != c("dNBR","geometry"))] # hopefully this works, removing geometry
model = Predictor$new(learner, data = x, y = data$dNBR)

# Feature Effects - ale plots
#      computes the effects for all given features on the model prediction
#      ale plots > pdp plots because they still work when features are correlated
effect = FeatureEffects$new(model, method = "ale") # accumulated local effects
plot(effect)

# Shapley
#      computes feature contributions for single predictions with the Shapley value
#      Given the current set of feature values, the contribution of a feature value to the difference between the actual              prediction and the mean prediction is the estimated Shapley value
x.interest = data.frame(x[1,])  
shapley = Shapley$new(model, x.interest = x.interest)
plot(shapley) # phi gives increase or decrease in probability given the values on vertical axis

# Feature Imp
#       Level of importance of the features
num_features = c("DOBrounded", "PlantAge", "BASAL_AREA", "CROWN_CLOS", "PineCov", "SpruceCov", "BroadBurn", "Brushed")
effect.imp = FeatureImp$new(model, loss = "mae") # mean absolute error
effect.imp$plot(features = num_features)

# Independent test data
train_set = sample(task$nrow, 0.8 * task$nrow)
test_set = setdiff(seq_len(task$nrow), train_set)
learner$train(task, row_ids = train_set)
prediction = learner$predict(task, row_ids = test_set)

# plot on training
model = Predictor$new(learner, data = (data[train_set,][which(names(data) != "geometry")]), y = "dNBR")
effect = FeatureImp$new(model, loss = "mae")
plot_train = plot(effect, features = num_features)

# plot on test data
model = Predictor$new(learner, data = (data[test_set,][which(names(data) != "geometry")]), y = "dNBR")
effect = FeatureImp$new(model, loss = "mae")
plot_test = plot(effect, features = num_features)

# combine into single plot
plot_train + plot_test

# Feature effects
model = Predictor$new(learner, data = (data[train_set,][which(names(data) != "geometry")]), y = "dNBR")
effect = FeatureEffects$new(model)
plot(effect, features = num_features)

model = Predictor$new(learner, data = (data[test_set,][which(names(data) != "geometry")]), y = "dNBR")
effect = FeatureEffects$new(model)
plot(effect, features = num_features)


#--- DALEX
#     Methods for analyzing the model at the level of a single prediction and the global level
model_exp = explain_mlr3(learner,
                         data = x, # provide data without y 
                         y = data$dNBR,
                         label = "Shovel",
                         colorize = FALSE)

# Dataset level exploration
# Importance of variables - permutation based importance
model_vi = model_parts(model_exp)
head(model_vi)
plot(model_vi, show_boxplots = FALSE)

# Partial dependence plots of top variables
selected_variables =c("PlantAge", "PineCov", "CROWN_CLOS", "BASAL_AREA", "SpruceCov", "conifCov", "decidCov", "Brushed")
pd = model_profile(model_exp, 
                   variables = selected_variables)$agr_profiles
pd
plot(pd) +
  scale_y_continuous("Estimated dNBR") +
  ggtitle("Partial Dependence profiles for selected variables")


```

# Island
Train a ranger model then do model interpretation
```{r Island, echo=FALSE}
rm(learner, rr, rrFlt)

#--- Step 1: Define a task
task = TaskRegrST$new("Island",
                       backend = Island_sf, 
                       target = "dNBR")


#--- Step 2: Define a learner
# default ranger parameters: https://mlr3learners.mlr-org.com/reference/mlr_learners_regr.ranger.html
learner = lrn("regr.ranger", 
              num.trees = 1000,
              importance = "permutation",
              respect.unordered.factors = "order",
              predict_type = "response")


#--- Step 3: Model validation
resampling = rsmp("repeated_spcv_coords", folds=10, repeats=2)
rr = resample(task, learner, resampling, store_models = TRUE, store_backends = FALSE)
rr$aggregate(msr("regr.rsq")) # by fold R2 (avg. perforamance over all k fold from CV)
rr$prediction()$score(msr("regr.rsq")) # global R2 (R2 over every data point predicted during CV)
# fold R2 = 0.24
# global R2 = 0.31


#--- Step 4: Train learner
learner$train(task)
learner$model # R2 on fully trained model
#R2oob = 0.45


#--- Step 5: Feature selection
resampling = rsmp("spcv_coords") 
measure = msr("regr.rsq")
terminator = trm("evals", n_evals=20)

instance = FSelectInstanceSingleCrit$new(
  task = task,
  learner = learner,
  resampling = resampling,
  measure = measure, 
  terminator = terminator
)

fselector = fs("random_search")
fselector$optimize(instance)
instance$result_feature_set


#--- Step 5: Subset task with selected features
task$select(instance$result_feature_set) # this subsets task, so need to reread original task if want to use it after this


#--- Step 6: Tuned model validation
resampling = rsmp("repeated_spcv_coords", folds=10, repeats=2)
rrFlt = resample(task, learner, resampling, store_models = TRUE, store_backends = FALSE)
rrFlt$aggregate(msr("regr.rsq")) # fold
rrFlt$prediction()$score(msr("regr.rsq")) # global
# fold R2 = 0.26
# global R2 = 0.31


#--- Step 7: Train learner
learner$train(task)
learner$model # R2 on fully trained model
#R2oob = 0.46


#--- Step 8: Model interpretation
#--- IML
data = as.data.frame(Island_sf)
x = data[which(names(data) != c("dNBR","geometry"))]
model = Predictor$new(learner, data = x, y = data$dNBR)


# Feature Effects - ale plots
#      computes the effects for all given features on the model prediction
#      ale plots > pdp plots because they still work when features are correlated
effect = FeatureEffects$new(model, method = "ale") # accumulated local effects
plot(effect)

# Shapley
#      computes feature contributions for single predictions with the Shapley value
#      Given the current set of feature values, the contribution of a feature value to the difference between the actual              prediction and the mean prediction is the estimated Shapley value
x.interest = data.frame(x[1,])  
shapley = Shapley$new(model, x.interest = x.interest)
plot(shapley) # phi gives increase or decrease in probability given the values on vertical axis

# Feature Imp
#       Level of importance of the features
num_features = c("DOBrounded", "PlantAge", "BASAL_AREA", "CROWN_CLOS", "PineCov", "SpruceCov", "BroadBurn", "Brushed")
effect.imp = FeatureImp$new(model, loss = "mae") # mean absolute error
effect.imp$plot(features = num_features)

# Independent test data
train_set = sample(task$nrow, 0.8 * task$nrow)
test_set = setdiff(seq_len(task$nrow), train_set)
learner$train(task, row_ids = train_set)
prediction = learner$predict(task, row_ids = test_set)

# plot on training
model = Predictor$new(learner, data = (data[train_set,][which(names(data) != "geometry")]), y = "dNBR")
effect = FeatureImp$new(model, loss = "mae")
plot_train = plot(effect, features = num_features)

# plot on test data
model = Predictor$new(learner, data = (data[test_set,][which(names(data) != "geometry")]), y = "dNBR")
effect = FeatureImp$new(model, loss = "mae")
plot_test = plot(effect, features = num_features)

# combine into single plot
plot_train + plot_test

# Feature effects
model = Predictor$new(learner, data = (data[train_set,][which(names(data) != "geometry")]), y = "dNBR")
effect = FeatureEffects$new(model)
plot(effect, features = num_features)

model = Predictor$new(learner, data = (data[test_set,][which(names(data) != "geometry")]), y = "dNBR")
effect = FeatureEffects$new(model)
plot(effect, features = num_features)


#--- DALEX
#     Methods for analyzing the model at the level of a single prediction and the global level
model_exp = explain_mlr3(learner,
                         data = x, # provide data without y 
                         y = data$dNBR,
                         label = "Island",
                         colorize = FALSE)

# Dataset level exploration
# Importance of variables - permutation based importance
model_vi = model_parts(model_exp)
head(model_vi)
plot(model_vi, show_boxplots = FALSE)

# Partial dependence plots of top variables
selected_variables =c("PlantAge", "PineCov", "CROWN_CLOS", "BASAL_AREA", "SpruceCov", "conifCov", "decidCov", "Brushed")
pd = model_profile(model_exp, 
                   variables = selected_variables)$agr_profiles
pd
plot(pd) +
  scale_y_continuous("Estimated dNBR") +
  ggtitle("Partial Dependence profiles for selected variables")


```

# Nadina
Train a ranger model then do model interpretation
```{r Nadina, echo=FALSE}
rm(learner, rr)

#--- Step 1: Define a task
task = TaskRegrST$new("Nadina",
                       backend = Nadina_sf, 
                       target = "dNBR")


#--- Step 2: Define a learner
# default ranger parameters: https://mlr3learners.mlr-org.com/reference/mlr_learners_regr.ranger.html
learner = lrn("regr.ranger", 
              num.trees = 1000,
              importance = "permutation",
              respect.unordered.factors = "order",
              predict_type = "response")


#--- Step 3: Model validation
resampling = rsmp("repeated_spcv_coords", folds=10, repeats=2)
rr = resample(task, learner, resampling, store_models = TRUE, store_backends = FALSE)
rr$aggregate(msr("regr.rsq")) # by fold R2 (avg. perforamance over all k fold from CV)
rr$prediction()$score(msr("regr.rsq")) # global R2 (R2 over every data point predicted during CV)
# fold R2 = 0.16
# global R2 = 0.23


#--- Step 4: Train learner
learner$train(task)
learner$model # R2 on fully trained model
#R2oob = 0.57


#--- Step 5: Feature selection
resampling = rsmp("spcv_coords") 
measure = msr("regr.rsq")
terminator = trm("evals", n_evals=20)

instance = FSelectInstanceSingleCrit$new(
  task = task,
  learner = learner,
  resampling = resampling,
  measure = measure, 
  terminator = terminator
)

fselector = fs("random_search")
fselector$optimize(instance)
instance$result_feature_set


#--- Step 5: Subset task with selected features
task$select(instance$result_feature_set) # this still subsets task, so need to reread original task if want to use it after this


#--- Step 6: Tuned model validation
resampling = rsmp("repeated_spcv_coords", folds=10, repeats=2)
rrFlt = resample(task, learner, resampling, store_models = TRUE, store_backends = FALSE)
rrFlt$aggregate(msr("regr.rsq")) # fold
rrFlt$prediction()$score(msr("regr.rsq")) # global
# fold R2 = 0.19
# global R2 = 0.26


#--- Step 7: Train learner
learner$train(task)
learner$model # R2 on fully trained model
#R2oob = 0.54


#--- Step 8: Model interpretation
#--- IML
data = as.data.frame(Nadina_sf)
x = data[which(names(data) != c("dNBR","geometry"))] 
model = Predictor$new(learner, data = x, y = data$dNBR)

# Feature Effects - ale plots
#      computes the effects for all given features on the model prediction
#      ale plots > pdp plots because they still work when features are correlated
effect = FeatureEffects$new(model, method = "ale") # accumulated local effects
plot(effect)

# Shapley
#      computes feature contributions for single predictions with the Shapley value
#      Given the current set of feature values, the contribution of a feature value to the difference between the actual              prediction and the mean prediction is the estimated Shapley value
x.interest = data.frame(x[1,])  
shapley = Shapley$new(model, x.interest = x.interest)
plot(shapley) # phi gives increase or decrease in probability given the values on vertical axis

# Feature Imp
#       Level of importance of the features
num_features = c("DOBrounded", "PlantAge", "BASAL_AREA", "CROWN_CLOS", "PineCov", "SpruceCov", "BroadBurn", "Brushed")
effect.imp = FeatureImp$new(model, loss = "mae") # mean absolute error
effect.imp$plot(features = num_features)

# Independent test data
train_set = sample(task$nrow, 0.8 * task$nrow)
test_set = setdiff(seq_len(task$nrow), train_set)
learner$train(task, row_ids = train_set)
prediction = learner$predict(task, row_ids = test_set)

# plot on training
model = Predictor$new(learner, data = (data[train_set,][which(names(data) != "geometry")]), y = "dNBR")
effect = FeatureImp$new(model, loss = "mae")
plot_train = plot(effect, features = num_features)

# plot on test data
model = Predictor$new(learner, data = (data[test_set,][which(names(data) != "geometry")]), y = "dNBR")
effect = FeatureImp$new(model, loss = "mae")
plot_test = plot(effect, features = num_features)

# combine into single plot
plot_train + plot_test

# Feature effects
model = Predictor$new(learner, data = (data[train_set,][which(names(data) != "geometry")]), y = "dNBR")
effect = FeatureEffects$new(model)
plot(effect, features = num_features)

model = Predictor$new(learner, data = (data[test_set,][which(names(data) != "geometry")]), y = "dNBR")
effect = FeatureEffects$new(model)
plot(effect, features = num_features)


#--- DALEX
#     Methods for analyzing the model at the level of a single prediction and the global level
model_exp = explain_mlr3(learner,
                         data = x, # provide data without y 
                         y = data$dNBR,
                         label = "Nadina",
                         colorize = FALSE)

# Dataset level exploration
# Importance of variables - permutation based importance
model_vi = model_parts(model_exp)
head(model_vi)
plot(model_vi, show_boxplots = FALSE)

# Partial dependence plots of top variables
selected_variables =c("PlantAge", "PineCov", "CROWN_CLOS", "BASAL_AREA", "SpruceCov", "conifCov", "decidCov", "Brushed")
pd = model_profile(model_exp, 
                   variables = selected_variables)$agr_profiles
pd
plot(pd) +
  scale_y_continuous("Estimated dNBR") +
  ggtitle("Partial Dependence profiles for selected variables")


```
